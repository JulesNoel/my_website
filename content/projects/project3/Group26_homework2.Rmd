---
title: "Session 4: Homework 2"
author: "Group 26"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
```



# Climate change and temperature anomalies 


```{r weather_data, cache=TRUE, include=FALSE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

Beforehand, we downloaded a data set on weather anomalies in the past. We will make some modifications to it and analyze it.
Let's start by selecting the months from January to December and drop the "periods" variables. 

```{r tidyweather}
tidyweather<- weather %>% 
  select(Year,Jan:Dec) %>% 
  pivot_longer(2:13, names_to="month", 
               values_to = "delta")
  
```

Our tidyweather data frame has now three variables now, one each for 

1. year, 
1. month, and 
1. delta, or temperature deviation.

## Plotting Information

Let us plot the data using a time-series scatter plot, and add a trendline. To do that, we first need to create a new variable called `date` in order to ensure that the `delta` values are plot chronologically. 


```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(alpha=0.8)+
  geom_smooth(color="red") +
  theme_bw() +
  labs (title = "Weather anomalies turning positive and increasing in the last 50 years",
    subtitle = "Weather Anomalies", 
    x = "Year", 
    y= "Temperature Delta"
  )

```

This trend could be different depending on the month. Thus we'll have a look at the same kind of graph but this time faceted by month. 

```{r facet_wrap}

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (title = "Weather anomalies changes are constant throughout the months",
    subtitle = "Weather Anomalies", x="Year", y="Temperature Delta"
  )+
  facet_wrap(~month)
  
```


It is sometimes useful to group data into different time periods to study historical data. For example, we often refer to decades such as 1970s, 1980s, 1990s etc. to refer to a period of time. NASA calculates a temperature anomaly, as difference form the base period of 1951-1980. The code below creates a new data frame called `comparison` that groups data in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010 and 2011-present. 

We remove data before 1800 and before using `filter`. Then, we use the `mutate` function to create a new variable `interval` which contains information on which period each observation belongs to. We can assign the different periods using `case_when()`.


```{r intervals, eval=TRUE}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```


Now that we have the `interval` variable, we can create a density plot to study the distribution of monthly deviations (`delta`), grouped by the different time periods we are interested in. 

```{r density_plot, eval=TRUE}

ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.2) +   #density plot with tranparency set to 20%
  theme_bw() +                #theme
  labs (
    title = "More recent periods see increasingly positive weather anomalies",
     subtitle = "Distribution of weather anomalies by time period",
    y     = "Density"         #changing y-axis label to sentence case
    ,x = "Temperature Delta"
  )

```

So far, we have been working with monthly anomalies. However, we might be interested in average annual anomalies. We can do this by using `group_by()` and `summarise()`, followed by a scatter plot to display the result. 

```{r averaging, eval=TRUE}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(annual_average_delta = mean(delta, na.rm=TRUE)) 

#plotting the data:
ggplot(average_annual_anomaly, aes(x=Year, y= annual_average_delta))+
  geom_point()+
  
  #Fit the best fit line, using LOESS method
  geom_smooth() +
  
  #change to theme_bw() to have white background + black frame around plot
  theme_bw() +
  labs (
    title = "Average Yearly Anomalies are constantly increasing",
    y     = "Average Annual Delta"
  )                         


```


## Confidence Interval for `delta`



We are now going to construct a confidence interval for the average annual delta since 2011, both using a formula and using a bootstrap simulation with the `infer` package.

```{r, calculate_CI_using_formula, eval=TRUE}

formula_ci <- comparison %>% 

  # choose the interval 2011-present
  filter(interval == "2011-present") %>%
  filter(!is.na(delta)) %>% 
  # na.omit() %>% 
 
  # what dplyr verb will you use? 

  # calculate summary statistics for temperature deviation (delta) 
  # calculate mean, SD, count, SE, lower/upper 95% CI
  summarise(mean_delta=mean(delta),
            sd_delta=sd(delta), 
            count=n(),
            t_critical = qt(0.975, count-1),
            se_delta = sd(delta)/sqrt(count),
            margin_of_error = t_critical * se_delta,
            delta_low = mean_delta - margin_of_error,
            delta_high = mean_delta + margin_of_error)
  # what dplyr verb will you use? 

#print out formula_CI
formula_ci %>% tbl_df %>% rmarkdown::paged_table()

```


```{r, calculate_CI_using_bootstrap}

# use the infer package to construct a 95% CI for delta
set.seed(1234)

boot_deltas <- comparison %>% 
  filter(interval == "2011-present") %>%
  filter(!is.na(delta)) %>% 
  specify(response = delta) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "mean")

percentile_ci <- boot_deltas %>% 
  get_confidence_interval(level = 0.95, type = "percentile") 



percentile_ci %>% tbl_df %>% rmarkdown::paged_table()

formula_ci %>% 
  select(delta_low, delta_high) %>% tbl_df %>% rmarkdown::paged_table()
```

The mean delta over the last 10 years is 0.966 - a significant change based on NASA's findings. Both confidence interval methods give almost comparable results. 
Looking at the bootstrap method, the 95% confidence interval is between 0.917 and 1.02. This means that when looking at weather anomalies, the vast majority will over around this "one-degree" change, meaning the climate is deregulated and this will have devastating consequences.

# General Social Survey (GSS)

In this assignment we analyze data from the **2016 GSS sample data**, using it to estimate values of *population parameters* of interest about US adults. 

```{r, read_gss_data, cache=TRUE}
gss <- read_csv(here::here("data", "smallgss2016.csv"), 
                na = c("", "Don't know",
                       "No answer", "Not applicable", "NA"))
```

We can see that many responses should not be taken into consideration, like "No Answer", "Don't Know", "Not applicable", "Refused to Answer".

We will be creating 95% confidence intervals for population parameters. The variables we have are the following:

- hours and minutes spent on email weekly. The responses to these questions are recorded in the `emailhr` and `emailmin` variables. For example, if the response is 2.50 hours, this would be recorded as emailhr = 2 and emailmin = 30.
- `snapchat`, `instagrm`, `twitter`: whether respondents used these social media in 2016
- `sex`: Female - Male
- `degree`: highest education level attained

## Instagram and Snapchat, by sex

First, we estimate the *population* proportion of Snapchat or Instagram users in 2016.

1. To see that, we create a new variable, `snap_insta` that is *Yes* if the respondent reported using any of Snapchat (`snapchat`) or Instagram (`instagrm`), and *No* if not. If the recorded value was NA for both of these questions, the value in our new variable should also be NA.

```{r, snap_insta}

snap_or_insta <- gss %>% 
           mutate(snap_insta = case_when(
             instagrm == 'Yes' ~ 'Yes', 
             snapchat == 'Yes' ~ 'Yes', 
             instagrm == 'No' & snapchat == 'No' ~ 'No'))

```

2. Now, the proportion of Yesâ€™s for `snap_insta` among those who 
answered the question, i.e. excluding NAs can be found through the below:

```{r, proportion_of_snap_or_insta}
snap_insta_prop <- snap_or_insta %>%
  summarise(proportion=count(snap_insta=="Yes")/count(!is.na(snap_insta)))
 
snap_insta_prop %>% tbl_df %>% rmarkdown::paged_table()

```

3. To better understand the distribution around this proportion we construct the CI formula for proportions, using 95% CIs for men and women who used either Snapchat or Instagram.

```{r, CIs_for_female_and_male}

# Calculate proportion for men and women who used snap_or_insta 
  all <- snap_or_insta %>% 
group_by(sex) %>% 
summarise ( proportion = count(snap_insta=='Yes')/count(!is.na(snap_insta)),
            sd = sqrt(proportion*(1-proportion)), 
            count=n(),
            z_critical = qnorm(0.975,0,1),
            se = sd/sqrt(count),
            margin_of_error = z_critical * se,
            ci_low = proportion - margin_of_error,
            ci_high = proportion + margin_of_error)

# Print out CI for women
all %>% tbl_df %>% rmarkdown::paged_table()

```

## Twitter, by education level

Looking at another social media, we estimate the *population* proportion of Twitter users by education level in 2016. 

1. First, we turn `degree` from a character variable into a factor variable. 

```{r, fatorize_degree}
degree_gss <- gss %>% 
  mutate(degree = factor (degree, level = c('Lt high school', 'High school', 'Junior college', 'Bachelor', 'Graduate'))) 
  

```

2. Then we create a  new variable, `bachelor_graduate` that is *Yes* if the respondent has either a `Bachelor` or `Graduate` degree. As before, if the recorded value for either was NA, the value in our new variable should also be NA.

```{r, bachelor_graduate}
bachelor_graduate_gss <- degree_gss %>% 
  mutate(bachelor_graduate = case_when(
    degree == 'Bachelor' ~ 'Yes',
    degree == 'Graduate' ~ 'Yes'
  ))

```

3. So in the end, who use Twitter among the bachelors and graduates? 

```{r, bachelor_graduate_twitter}

# Calculate the proportion
bachelor_graduate_twitter <- bachelor_graduate_gss %>% 
  filter(!is.na(bachelor_graduate)) %>% 
  filter(!is.na(twitter)) %>% 
  summarise(proportion_yes=count(twitter=='Yes')/count(bachelor_graduate),
            proportion_no=count(twitter=='No')/count(bachelor_graduate))

# Print out the proportion
bachelor_graduate_twitter %>% tbl_df %>% rmarkdown::paged_table()
```
4. Again, we want a better idea of the distribution of our data, so we construct two 95% CIs for `bachelor_graduate` vs whether they use (Yes) and don't (No) use twitter.

```{r, CIs_for_Yes_and_No,warning=FALSE}

# Construct CIs for `bachelor_graduate` using twitter
bachelor_graduate_yes <- bachelor_graduate_gss %>% 
  filter(!is.na(bachelor_graduate)) %>% 
  filter(!is.na(twitter)) %>% 
  summarise(proportion_yes=count(twitter=='Yes')/count(bachelor_graduate),
            sd_yes=sqrt((proportion_yes)*(1-proportion_yes)),
            count=n(),
            se_yes=sd_yes/sqrt(count),
            z_critical = qnorm(0.975,0,1),
            margin_of_error_yes=z_critical*se_yes,
            ci_low_yes=proportion_yes-margin_of_error_yes,
            ci_high_yes=proportion_yes+margin_of_error_yes)


# Print out CIs for `bachelor_graduate` using twitter
bachelor_graduate_yes %>% 
  select(ci_low_yes,ci_high_yes) %>% tbl_df %>% rmarkdown::paged_table()

# Construct CIs for `bachelor_graduate` not using twitter
bachelor_graduate_no <- bachelor_graduate_gss %>% 
  filter(!is.na(bachelor_graduate)) %>% 
  filter(!is.na(twitter)) %>% 
  summarise(proportion_no=count(twitter=='No')/count(bachelor_graduate),
            sd_no=sqrt((proportion_no)*(1-proportion_no)),
            count=n(),
            se_no=sd_no/sqrt(count),
            z_critical = qnorm(0.975,0,1),
            margin_of_error_no=z_critical*se_no,
            ci_low_no=proportion_no-margin_of_error_no,
            ci_high_no=proportion_no+margin_of_error_no)

# Print out CIs for `bachelor_graduate` not using twitter
bachelor_graduate_no %>% 
  select(ci_low_no, ci_high_no) %>% tbl_df %>% rmarkdown::paged_table()

```

According to answers above, these two Confidence Intervals don't overlap.


## Email usage

Social medias are something, but these old school email are still one of the most prominenet channel of communication. It could be interesting to know long at the usage of email per week on average.

1. To merge the minutes and hours in our data frame, we a new variable called `email` that combines `emailhr` and `emailmin` to reports the number of minutes the respondents spend on email weekly.

```{r, email_minute}
gss_email <- gss %>% 
  mutate(email=(emailhr*60)+emailmin)

```

2. Another way to explore the distribution of a variable is to simply plot the density so we find the mean and the median number of minutes respondents spend on email weekly.

```{r, visualise_email}
# Plot the distribution of minutes spent on email weekly
ggplot(gss_email, aes(x=email))+
  geom_density()+
  labs(title="Distribution of minutes spent on email weekly",
       x="Minutes",
       y="Density")

# Calculate mean and median number of minutes
mean_median <- gss_email %>% 
  filter(!is.na(email)) %>% 
  summarise(mean=mean(email),
            median=median(email))

#  Print out mean and median
mean_median %>% tbl_df %>% rmarkdown::paged_table()


```
Although we have calculated both the mean and median, it is more realistic to look at the median when analyzing this data. Indeed, as the distribution is highly skewed, the mean will not provide a reliable view of the situation.


3. One last time, we want to understand better the email usage per week. So let's do our ususal 95% confidence interval and discuss our results.

```{r, bootstrap_for_email}

# Use the infer package to construct a 95% CI for email
set.seed(1234)

boot_email <- gss_email %>% 
  filter(!is.na(email)) %>% 
  specify(response=email) %>% 
  generate(reps=1000, type="bootstrap") %>% 
  calculate(stat="mean")

percentile_ci_email <- boot_email %>% 
  get_confidence_interval(level=0.95, type="percentile")

# Convert the endpoints in humanized units
ci_in_humanized_units <- percentile_ci_email %>% 
  mutate(lower_ci_hour=lower_ci%/%60,
         lower_ci_minute=round(lower_ci%%60,0),
         upper_ci_hour=upper_ci%/%60,
         upper_ci_minute=round(upper_ci%%60,0))
#ci_in_humanized_units

# Print out humanized results
paste("Lower ci: ", ci_in_humanized_units$lower_ci_hour,"hours and", ci_in_humanized_units$lower_ci_minute, "minutes")

paste("Upper ci: ", ci_in_humanized_units$upper_ci_hour,"hours and", ci_in_humanized_units$upper_ci_minute, "minutes")

```

The 99% confidence interval would be wider. As the level of confidence of having a number captured in an interval, it needs to be wider.


# Trump's Approval Margins



```{r, cache=TRUE, include=FALSE}
# Import approval polls data

approval_polllist <- read_csv('https://projects.fivethirtyeight.com/trump-approval-data/approval_polllist.csv') 

glimpse(approval_polllist)

# Use `lubridate` to fix dates, as they are given as characters.
approval_polllist_fixdates <- approval_polllist %>% 
  mutate(timestamp = parse_date_time(timestamp, orders = "HMSdmy"),modeldate = mdy(modeldate), startdate = mdy(startdate), enddate = mdy(enddate), createddate = mdy(createddate))

```

## Create a plot

No we are going to calculate the average net approval rate (approve- disapprove) for each week since Donald Trump got into office. 

```{r trump_margins, out.width="100%"}

graph_data <- approval_polllist_fixdates %>%  
  filter(subgroup=="Voters") %>% #To reflect only the people that will have an impact during the election.
  mutate(week = week(enddate), year = year(enddate)) %>% #setting the date ass the end of the poll
  mutate(average_net = adjusted_approve - adjusted_disapprove, year = year(enddate)) %>% #calculating the average net approval using the adjusted data for more precision.
  group_by(week, year) %>% 
  summarise(mean_approval = mean(average_net),
            lower = mean(average_net) - qt(0.975, (n() - 1))*sd(average_net)/sqrt(n()),
            upper = mean(average_net) + qt(0.975, (n() - 1))*sd(average_net)/sqrt(n())) #confidence interval using 95% 
  
#constructing the plot
graph <- ggplot(graph_data, 
                aes(x=week,
                    y=mean_approval, 
                    color = as.factor(year))) + 
  geom_line(alpha=0.6) + 
  geom_point(alpha=0.6) + #making the colors lighter by decreasing the opacity
  geom_hline(yintercept=0, color = "orange") + #setting up the line at 0
  geom_ribbon(aes(ymin=lower, ymax=upper), linetype=1, alpha=0.07) +
  labs(title = "Estimating Net Approval (approve-disapprove) for Donald Trump", #Replicating the exact same texts.
       subtitle = "Weekly average of all polls", 
       y = "Average Net Approval (%)", 
       x = "Week of the year") +
  facet_wrap(~year) +
  theme_bw() +
  theme(legend.position="none") +#removing the legend
  scale_color_manual(values = c("brown1","darkolivegreen3","darkturquoise","darkorchid1")) + 
  scale_y_continuous(labels = scales::number_format(accuracy = 0.1, decimal.mark = '.'), 
              breaks = c(7.5, 5, 2.5,0.0,-2.5, -5.0, -7.5, -10.0, -12.5, -15.0, -17.5, -20.0)) +# Setting up the intervals on the axis
  scale_x_continuous(breaks = c(0, 13, 26, 39, 52))

ggsave("graph_trump.png", width = 12, height = 5, dpi = 600) #Save the graph locally

knitr::include_graphics(here::here("graph_trump.png")) #Display the graph

```

## Compare Confidence Intervals

The confidence interval from `week 15` (6-12 April 2020) to `week 34` (17-23 August 2020) widens. Showing that Trump's supporter and disapprovers further diverged as the evolving economic situation, social issues (BLM), and pandemic contribute to uncertainty. Such delicate subjects often force the president to take one side or another. As these sides are recently quite decisive, his decisions have a more profound impact on the approval ratings from the people. 



# Gapminder revisited


```{r, get_data, cache=TRUE, include=TRUE}

# load gapminder HIV data
hiv <- read_csv(here::here("data","adults_with_hiv_percent_age_15_49.csv"))
life_expectancy <- read_csv(here::here("data","life_expectancy_years.csv"))

# get World bank data using wbstats
indicators <- c("SP.DYN.TFRT.IN","SE.PRM.NENR", "SH.DYN.MORT", "NY.GDP.PCAP.KD")


library(wbstats)

worldbank_data <- wb_data(country="countries_only", #countries only- no aggregates like Latin America, Europe, etc.
                          indicator = indicators, 
                          start_date = 1960, 
                          end_date = 2016)

# get a dataframe of information regarding countries, indicators, sources, regions, indicator topics, lending types, income levels,  from the World Bank API 
countries <-  wbstats::wb_cachelist$countries

```

We have to join the 3 dataframes (life_expectancy, worldbank_data, and HIV) into one and clean properly before use.

``` {r, cache=TRUE}

# We have chosen to use pivot_longer to reformat the hiv and life_expectancy data frames, as this matches better the format of worldbank_data, therefore allowing us to record multiple indicators. Additionally, given that there is little data prior to 1985, we have decided to only keep subsequent data points.

clean_hiv <- hiv %>% 
  select(country, "1990":"2011") %>% #taking the first year with relevant data and complete info for all countries - 1990
  pivot_longer(cols="1990":"2011", names_to="date", values_to="hiv_rate") %>% 
  transform(date = as.double(date))

clean_life_expectancy <- life_expectancy %>% 
  select(country, "1960":"2016") %>% 
  pivot_longer(cols="1960":"2016", names_to="date", values_to="life_expectancy") %>% 
  transform(date = as.double(date))

# We decided to use left_join for compiling the data sets as the merge returns all of the rows from one table and any matching rows from the second table. However, we used inner_join for the countries in order to not have duplicate data as the countries appear in both data sets.

combined_world <- worldbank_data %>%
  rename(gdpPCap = NY.GDP.PCAP.KD,
         fertility_rate = SP.DYN.TFRT.IN,
         school_enrollment = SE.PRM.NENR,
         mortality_under5 = SH.DYN.MORT) %>%
  left_join(clean_life_expectancy, by = c("date","country")) %>% 
  inner_join(countries, by = "country") %>%
  left_join(clean_hiv, by = c("date","country")) %>% 
  transform(date = as.double(date))

```

1. Let's explore the relationship between HIV prevalence and life expectancy.

``` {r, cache=TRUE}

hiv_life_expectancy <- combined_world %>% 
  filter(hiv_rate != is.na(hiv_rate)) %>% 
  filter(life_expectancy != is.na(life_expectancy)) %>% 
  select(life_expectancy, hiv_rate, date)

ggplot(hiv_life_expectancy,
       aes(x=hiv_rate,
           y=life_expectancy)) +
 geom_point(alpha=0.4) +
  scale_x_log10()+
 geom_smooth() +
  facet_wrap(~date) +
 theme_bw() + 
  labs(title = "Higher HIV rate leads to lower life expectancy", 
       subtitle = "Relationship between HIV prevalence and life expectancy",
       x = "HIV Rate", 
       y = "Life expectancy")

```

According to the graphs above, there is a negative trend between life expectancy and HIV rate, meaning that the higher the HIV rate, the lower the life expectancy. However, given that the data above does not give an insight into the differences among the regions, we have decided to also look at the correlation between HIV rate and life expectancy by region.


``` {r, cache=TRUE}

hiv_and_life_expectancy <- combined_world %>%
  group_by(date, region) %>%
  drop_na(hiv_rate, life_expectancy) %>%
  summarise(correlation = cor(hiv_rate, life_expectancy))

ggplot(hiv_and_life_expectancy, 
       aes(x = date, 
           y = correlation)) +
  geom_point() +
  xlim(1990,2011) +
  facet_wrap(~region, scales = "free") +
  geom_smooth() +
  labs(title = "Correlation of Life Expectancy and HIV Rates over Time",
       x = "Year",
       y = "Correlation Coefficient") +
  theme_bw()
```

The data shows that there is an inverse relationship between life expectancy and HIV rates throughout the world - although there seems to be limited data for North America. The inverse relationship appears to be much stronger in East Asia & Pacific as well as Sub-Saharan Africa, meaning that as the HIV rate decreases, life expectancy increases. 

2. We will also have a look at the relationship between fertility rate and GDP per capita.

``` {r, cache=TRUE}

combined_world %>%
  ggplot(aes(y = fertility_rate, x = gdpPCap)) +
  geom_point() +
  facet_wrap(~region, scales = "free") +
  scale_x_log10()+
  scale_y_log10()+
  geom_smooth() +
  theme_bw() +
  labs(title = "Relationship between fertility rate and GDP per capita",
       y = "Fertility Rate",
       x = "GDP per Capita")
```

The relationship between fertility rate and GDP per capita appears to be negatively correlated according to the data provided, meaning that the higher the fertility rate, the lower the income. This seems to be especially true for the Latin American & Caribbean region.


3.Unfortunately, the data is not complete. We will see below which region has the most missing entries for the HIV variable.

``` {r, cache=TRUE}

HIV_missing <- combined_world %>%
  group_by(region) %>%
  summarise(missing_HIV_rate = sum(is.na(hiv_rate)))

ggplot(HIV_missing, aes(x = missing_HIV_rate, y = reorder(region, missing_HIV_rate))) +
  geom_col() +
  theme_wsj() +
  labs(title = "Most missing data for HIV are from Europe & Central Asia", subtitle="Count HIV missing values per region",
       x = "No. of missing entries",
       y = "")+ 
  theme(plot.title = element_text(size=10))+
  theme(plot.subtitle = element_text(size=8))+
  theme(axis.text=element_text(size=8))

```

4. We know that the mortality rates usually improved since 1990. However, we are interested in the five best and worst performers in each region as a percentage change since 1990. 
We take 1990 as the base year because it is the first year with enough data for the majority of the countries present in our data set. 

``` {r, cache=TRUE}
five_year_mortality <- combined_world %>%
  group_by(region, date) %>%
  summarise(mean_mortality = mean(mortality_under5, na.rm = TRUE))

ggplot(five_year_mortality, aes(x = date, y = mean_mortality)) +
  geom_point(alpha=0.5) +
  facet_wrap(~region, scales = "free") +
  theme_bw() +
  labs(title = "All regions experienced a fall in child mortality rates",
       subtitle = "Mortality rate in children under 5 change over 50 years",
       y = "Mortality rate of children under 5",
       x = "")
 
mortality_top_five <- combined_world %>%
  filter(date == "1990" | date == "2011") %>% #taking the first year with relevant data and complete info for all countries - 1990
  filter(mortality_under5 != is.na(mortality_under5)) %>% 
  select(region, country, date, mortality_under5) %>%
  pivot_wider(names_from = date, values_from = mortality_under5) %>%
  rename(start = 3,
         end = 4) %>%
  mutate(change = (end - start)/start*100) %>%
  arrange(region, change, na.rm = TRUE) %>%
  group_by(region) %>%
  slice_head(n=5)

mortality_bottom_five <- combined_world %>%
  filter(date == "1990" | date == "2011") %>% #taking the first year with relevant data and complete info for all countries - 1990
   filter(mortality_under5 != is.na(mortality_under5)) %>% 
  select(region, country, date, mortality_under5) %>%
  pivot_wider(names_from = date, values_from = mortality_under5) %>%
  rename(start = 3,
         end = 4) %>%
  mutate(change = (end - start)/start*100) %>%
  arrange(region, change, na.rm = TRUE) %>%
  group_by(region) %>%
  slice_tail(n=5)


plot_1 <- ggplot(mortality_top_five, aes(x = change, y = reorder(country, change))) + 
  geom_col(fill = "chartreuse3") + 
  geom_smooth(method = "lm") +
  facet_wrap(~region, scales = "free") +
  labs(y = "", x = "Percentage change in mortality rate since 1990", title="Best improvements in the mortality rates") + 
  scale_x_reverse()


ggsave("Plot_1.png", width = 10, height = 6, dpi = 200)
knitr::include_graphics(here::here("Plot_1.png"), error = FALSE)

plot_2 <- ggplot(mortality_bottom_five, aes(x = change, y = reorder(country, change))) + 
  geom_col(fill = "brown3") + 
  geom_smooth(method = "lm") +
  facet_wrap(~region, scales = "free") +
  labs(y = "", x = "Percentage change in mortality rate since 1990", title="Some nations even experienced increased in their mortality rate under 5") + 
  scale_x_reverse()

ggsave("Plot_2.png", width = 10, height = 6, dpi = 200)
knitr::include_graphics(here::here("Plot_2.png"), error = FALSE)

```

5. Is there a relationship between primary school enrollment and fertility rate?

``` {r, cache=TRUE}
combined_world %>%
  
  ggplot(aes(x = fertility_rate, y = school_enrollment)) +
  geom_point() +
  facet_wrap(~region, scales = "free") +
  theme_bw() +
  geom_smooth() +
  labs(subtitle = "Relationship between School Enrollment Rates and Fertility Rates", title="School enrollment rates and fertility rates are negatively related",
       x = "Fertility Rate",
       y = "% School Enrollment Rate")
``` 

From the data above, a negative trend line can be observed for most regions, although for North America this is hard to confirm as there are limited data points. Nevertheless, a negative trend line suggests that the more children women have, i.e. higher fertility rate, the less likely they are to send their children to primary school, possibly due to fewer resources per child. This is especially evident in Sub-Saharan Africa which is generally a poorer nation.


# Challenge 1: CDC COVID-19 Public Use Data



```{r, cache=TRUE, include= FALSE}
# URL link to CDC to download data
url <- "https://data.cdc.gov/api/views/vbim-akqf/rows.csv?accessType=DOWNLOAD"

covid_data <- vroom(url)%>%
  clean_names()

glimpse(clean_names)
        
```

We are interested in learning more about the death rate among covid patients. Given the data we have, we would like to produce two graphs that show death % rate:

1. by age group, sex, and whether the patient had co-morbidities or not
1. by age group, sex, and whether the patient was admited to Intensive Care Unit (ICU) or not.


```{r covid_challenge, echo=FALSE, out.width="100%", include=FALSE}
knitr::include_graphics(here::here("images", "covid_death_rate_comorbidities.png"), error = FALSE)
knitr::include_graphics(here::here("images", "covid_death_rate_icu.png"), error = FALSE)
```

Before we start the analysis we have to get rid of data entries that don't give us any information.

```{r}

# Cleaning the data set by replacing "Unknown" and "Missing" values with "NA"

covid_data <- covid_data %>% 
  mutate(icu_yn = case_when(
    icu_yn == "Unknown" ~ NA_character_,
    icu_yn == "Missing" ~ NA_character_,
    TRUE ~ icu_yn
    ), death_yn = case_when(
    death_yn == "Unknown" ~ NA_character_,
    death_yn == "Missing" ~ NA_character_,
    TRUE ~ death_yn
    ), medcond_yn = case_when(
    medcond_yn == "Unknown" ~ NA_character_,
    medcond_yn == "Missing" ~ NA_character_,
    TRUE ~ medcond_yn
    ), sex = case_when(
    sex == "Unknown" ~ NA_character_,
    sex == "Missing" ~ NA_character_,
    sex == "Other" ~ NA_character_,
    TRUE ~ sex
    ), age_group = case_when(
    age_group == "Unknown" ~ NA_character_,
    age_group == "Missing" ~ NA_character_,
    TRUE ~ age_group))

```


```{r}

# we group the data by icu_yn to determine the effects of COVID across different groups of people

covid_data_grouped_by_medcond <- covid_data %>%
  group_by(age_group, sex, medcond_yn) %>% 
  filter(death_yn != is.na(death_yn) & sex != is.na(sex) & medcond_yn != is.na(medcond_yn) & age_group != is.na(age_group)) %>% 
  summarise(death_rate = count(death_yn=="Yes") / (count(death_yn=="Yes") + count(death_yn=="No"))) 

```

```{r}

# To make our data easier to understand we change the entries in medcond_yn

covid_data_grouped_by_medcond$medcond_yn <- factor(covid_data_grouped_by_medcond$medcond_yn, levels = c("Yes", "No"), 
                  labels = c("With comorbidities", "Without comorbidities"))

```

After arranging, we can plot our data to make better inferences

```{r}

graph_1<-ggplot(covid_data_grouped_by_medcond, aes(x=death_rate, y=age_group)) + 
  geom_col(aes(show.legend = FALSE), fill="cornflowerblue", alpha=0.7) + 
  facet_grid(medcond_yn ~ sex) +
  theme_bw() +
  ggtitle("Covid death % by age group, sex and presence of co-morbidities") +
  geom_text(aes(label = round(death_rate * 100,1), hjust = -0.1)) +
  theme(axis.title.x = element_blank()) + 
  theme(axis.title.y = element_blank()) +
  theme(legend.position = "none") + 
  labs(caption = "Source: CDC")
  
ggsave("Co-morbid.png", width = 20, height = 12, dpi = 200)
knitr::include_graphics(here::here("Co-morbid.png"), error = FALSE)
```
It can be seen that there is a clear correlation between age and mortality rate. The same holds true for earlier diagnosis of co-morbidities and mortality rate. Another interesting inference is that there is a higher probability of men dying from COVID than of women.

```{r}

# we group the data by icu_yn to determine the effects of COVID across different groups of people

covid_data_grouped_by_icu <- covid_data %>%   
  group_by(age_group, sex, icu_yn) %>% 
  filter(death_yn != is.na(death_yn) & sex != is.na(sex) & icu_yn != is.na(icu_yn) & age_group != is.na(age_group)) %>% 
  summarise(death_rate = count(death_yn=="Yes") / (count(death_yn=="Yes") + count(death_yn=="No")))

```

```{r}

# To make our data easier to understand we change the entries in icu_yn

covid_data_grouped_by_icu$icu_yn <- factor(covid_data_grouped_by_icu$icu_yn, levels = c("Yes", "No"), 
                  labels = c("Admitted to ICU", "No ICU"))

```

In our second plot we look at correlation of covid deaths and admittance to the intensive care unit

```{r}

graph_2 <- ggplot(covid_data_grouped_by_icu, aes(x=death_rate, y=age_group)) + 
  geom_col(aes(show.legend = FALSE), fill="coral1", alpha = 0.7) + 
  facet_grid(icu_yn ~ sex) +
  #facet_wrap(~sex + icu_yn, scale="free") + 
  theme_bw() +
  ggtitle("Covid death % by age group, sex and whether patient was admitted to ICU") +
  geom_text(aes(label = round(death_rate * 100,1), hjust = -0.1)) +
  theme(axis.title.x = element_blank()) + theme(axis.title.y = element_blank()) +
  theme(legend.position = "none") + 
  labs(caption = "Source: CDC")


ggsave("ICU_or_not.png", width = 20, height = 12, dpi = 200)
knitr::include_graphics(here::here("ICU_or_not.png"), error = FALSE)

```
Here it can be clearly seen that on average a much higher percentage of people who were admitted to the ICU died. Again we can also see that men and older people are more likely to die.


# Challenge 2: Excess rentals in TfL bike sharing

```{r, get_tfl_data, cache=TRUE, include=FALSE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```



```{r tfl_month_year_grid, echo=FALSE, out.width="100%", include=FALSE}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

Look at May and Jun and compare 2020 with the previous years. What's happening?

Due to the lockdown caused by the COVID19 pandemic, the population that would be in the streets or commuting to work using Boris Bikes has plummeted and this is the reason why the density curves for the months of April, May and June are more skewed to the left and with a lower kurtosis, showing that the frequence and "intensity" of usage of the Santander bikes scheme has lowered during this global pandemic.


However, the challenge was to reproduce some graphs - here are our attempts to that:

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%", include=FALSE}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```
```{r tfl_absolute_monthly_change-GROUP26"}

# First we filter the data from 2015 and on
bike_monthly <- bike %>% 
  filter(year>="2015")


# Then we group by month and year to summarise by the average of bikes hired in each month of the data
bike_monthly <- bike_monthly %>% 
  group_by(year,month) %>% 
  summarise(bikes_hired = median(bikes_hired))


# After that we group only by month to create a colunm "Average" that is related to average of bikes rented in each month
bike_monthly<- bike_monthly %>% 
  group_by(month) %>% 
  mutate(average= median(bikes_hired))


# Then, we create a column called "Excess_rentals" to know by how many rentals of bikes the month exceeded or was below the expected rentals, which is the average for that month
bike_monthly <- bike_monthly %>% 
  mutate(excess_rentals = bikes_hired - average) %>% 
  ungroup()

# We used the function interpolate in order to make the graphic easier to be comprehended 
bike_interpolated <- bike_monthly  %>% 
  #We splitted the data for each year
  split(.$year) %>% 
  
  #Now we use the map_df function to create columns called year.x, year.y, nat.x and nat.y
  map_df(~data.frame(year = approx(.x$month, .x$bikes_hired, n = 100), 
                     #interpolated months and avg hire
                     nat = approx(.x$month, .x$average, n = 100), 
                     year = .x$year[1],
                     month = .x$month[1]))

#We will use this information to plot the graph in the right order of months and with the exact same name 
month_label <- c("Jan", "Feb", "Mar",
           "Apr", "May", "Jun",
           "Jul", "Aug","Sep", 
           "Oce", "Nov", "Dec")

#After treating the data, we plot the graph, divided by year 
excess_rentals_graph1 <- ggplot(bike_interpolated, aes(x = nat.x,y= nat.y)) +  facet_wrap(~year) +
  geom_line(color = "#0019f6", size = 0.75) +
  geom_line(aes(nat.x, year.y), color = "black") +
  #No we fill the area between the two plots by using geom_ribbon
  geom_ribbon(aes(ymin = nat.y, ymax = pmin(year.y, nat.y)), fill = "#deafb1") +
  geom_ribbon(aes(ymin = year.y, ymax = pmin(year.y, nat.y)), fill = "#beebc2") +
  #Now we change the theme
  theme_minimal() +
  #After we change the x axis to make in continuous with the interpolated data and using the above mentioned labels
  scale_x_continuous(breaks= c(1,2,3,4,5,6,7,8,9,10,11,12),
        labels=month_label) +
  #Then we add the titles
  labs(title = "Monthly changes in TfL bike rentals",
       subtitle = "Change from monthly average shown in blue \nand calculated between 2015-2019",
       caption = "Source: TfL, London Data Store",
       y = "Bike rentals",
       x = "") +
  #Lastly, we make the titles bold, when necessary
  theme(plot.title = element_text(face = "bold"),
        axis.title = element_text(face = "bold"),
        plot.caption = element_text(face = "bold"))

# This line saves the graph as a .jpg file
ggsave("excess_rentals_graph1.jpg",plot=excess_rentals_graph1,width = 20,height = 8)

# And this line includes the above mentioned picture in the HTML knited file
knitr::include_graphics(here::here("excess_rentals_graph1.jpg"))

  
```

The second one looks at percentage changes from the expected level of weekly rentals. The two grey shaded rectangles correspond to the second (weeks 14-26) and fourth (weeks 40-52) quarters.

```{r tfl_percent_change, echo=FALSE, out.width="100%",include=FALSE}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```
```{r tfl_absolute_weekly_change-GROUP26",out.width="100%"}

# First we filter the data from 2015 and on
bike_weekly <- bike %>% 
  filter(year>="2015")

# Then we group by week and year to summarise by the average of bikes hired in each week of the data
bike_weekly <- bike_weekly %>% 
  group_by(year,week) %>% 
  summarise(bikes_hired = median(bikes_hired))

# After that we group only by month to create a colunm "Average" that is related to average of bikes rented in each week
bike_weekly<- bike_weekly %>% 
  group_by(week) %>% 
  mutate(average= median(bikes_hired))


# Then, we create a column called "Rentals_change" to know by how much the weekly rentals has changed compared to the average
bike_weekly <- bike_weekly %>% 
  mutate(rentals_change = ((bikes_hired - average)/average)) %>% 
  ungroup() %>% 
  
  #This line of code creates a column that we will use to know if the change of rentals was positive (Above) or negative (Below)
   mutate(change = ifelse(rentals_change>=0, "Above", "Below")) %>% 

#This line of code creates another column that will be used to shade the graph depending on the week
  mutate(shade = if_else(week <=13|week>=26&week<=39,"white","grey")) %>% 
  
  #This line of code creates another column that will be used to change the colour of the rug depending on the column "Change"
  mutate(rug_colour = if_else(change=="Below","#DEAFB1","#BEEBC2"))


#After treating the data, we plot the graph, divided by year 
excess_rentals_graph2 <- ggplot(bike_weekly, aes(x = week,y= rentals_change)) +
  geom_line()+
  
   #No we fill the area between the two plots by using geom_ribbon
  geom_ribbon(aes(ymin = 0, ymax = pmin(0,rentals_change), fill = "Above")) + 
geom_ribbon(aes(ymin = rentals_change, ymax = pmin(0,rentals_change), fill = "Below"))+
  facet_wrap(~year) +
  
  # This line of code changes the filling of the shade of the graph
  geom_tile(aes(fill = shade),
            width = 1, height = Inf, alpha = 0.3)+ 
  
  # Here we assign the column "rug color" to the color to the rug that will be used
  geom_rug(color = bike_weekly$rug_colour,sides="b") +
  
#Now we assign the colors that will be used in the plot
  scale_fill_manual(values = c("#DEAFB1","#BEEBC2","grey","white"))+
  
  theme_minimal()+
  #Including breaks to the graph
  scale_x_continuous(breaks=seq(13, 53, 13))+
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + 
  
  #In these three lines we change and customize the theme used
  theme(axis.ticks = element_blank())+
  theme(legend.position = "none") + 
  theme(panel.border = element_blank())+
  
  # Then we add the names of the Titles
  labs(x = "", y = "", title = "Weekly changes in TfL bike rentals", subtitle = "% change from weekly averages \ncalculated between 2015-2019", caption = "Source: TfL, London Data Store")+
 coord_fixed(ratio = 25)

# This line saves the graph as a .jpg file
ggsave("excess_rentals_graph2.jpg",plot=excess_rentals_graph2,width = 10,height = 8)


# # And this line includes the above mentioned picture in the HTML knited file
knitr::include_graphics(here::here("excess_rentals_graph2.jpg"))

```
In our opinion, we should use the median to calculate the expected rentals. Just as it was pointed in the graph "Distribution of bikes hired per month", there are months that are completely skewed or that have a very low kurtosis (April to June of 2020), then, if we would use the average, the impact of these "outliers" would affect our data and produce results that don't really express the central tendency(median) about the hiring of Boris Bikes.



# Details

- Who did you collaborate with: Group 26
- Approximately how much time did you spend on this problem set: 25-30 hours total
- What, if anything, gave you the most trouble: Gapminder Revisited